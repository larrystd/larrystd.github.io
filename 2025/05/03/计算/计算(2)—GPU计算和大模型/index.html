<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>计算(2)——GPU计算和大模型 | Infinity Code</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="https://unpkg.com/normalize.css"><link rel="stylesheet" type="text/css" href="https://unpkg.com/purecss/build/pure-min.css"><link rel="stylesheet" type="text/css" href="https://unpkg.com/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="https://unpkg.com/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="https://unpkg.com/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script type="text/javascript" src="https://unpkg.com/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="https://unpkg.com/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="https://unpkg.com/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">计算(2)——GPU计算和大模型</h1><a id="logo" href="/.">Infinity Code</a><p class="description">Simplicity is the soul of efficiency.</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">计算(2)——GPU计算和大模型</h1><div class="post-meta">2025-05-03<span> | </span><span class="category"><a href="/categories/compute/">compute</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#GPU-%E8%BF%90%E7%AE%97"><span class="toc-number">1.</span> <span class="toc-text">GPU 运算</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#CUDA-GPU%E5%B9%B6%E8%A1%8C%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.</span> <span class="toc-text">CUDA GPU并行编程模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#pytorch-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6"><span class="toc-number">1.2.</span> <span class="toc-text">pytorch 深度学习框架</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#cudnn%E5%92%8Cnccl"><span class="toc-number">1.3.</span> <span class="toc-text">cudnn和nccl</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#transformer%E5%92%8Cself-attention"><span class="toc-number">2.</span> <span class="toc-text">transformer和self-attention</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">2.1.</span> <span class="toc-text">1. 模型参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%8F%82%E6%95%B0%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90"><span class="toc-number">2.2.</span> <span class="toc-text">2. 参数显存分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-kvcache%E5%88%86%E6%9E%90"><span class="toc-number">2.3.</span> <span class="toc-text">3. kvcache分析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8A%A0%E9%80%9F"><span class="toc-number">3.</span> <span class="toc-text">大模型加速</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9"><span class="toc-number">3.1.</span> <span class="toc-text">1. 模型压缩</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E8%AE%A1%E7%AE%97%E4%BE%A7%E5%8A%A0%E9%80%9F"><span class="toc-number">3.2.</span> <span class="toc-text">2. 计算侧加速</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E5%92%8CIO%E4%BC%98%E5%8C%96"><span class="toc-number">3.3.</span> <span class="toc-text">内存和IO优化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6"><span class="toc-number">4.</span> <span class="toc-text">大模型推理框架</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#vLLM"><span class="toc-number">4.1.</span> <span class="toc-text">vLLM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TensorRT"><span class="toc-number">4.2.</span> <span class="toc-text">TensorRT</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MOE%E8%AE%AD%E7%BB%83"><span class="toc-number">5.</span> <span class="toc-text">MOE训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#deepseek%E5%BC%80%E6%BA%90"><span class="toc-number">6.</span> <span class="toc-text">deepseek开源</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-FlashMLA"><span class="toc-number">6.1.</span> <span class="toc-text">1. FlashMLA</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-DeepEP"><span class="toc-number">6.2.</span> <span class="toc-text">2. DeepEP</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-DeepGEMM"><span class="toc-number">6.3.</span> <span class="toc-text">3. DeepGEMM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-DualPipe-EPLB"><span class="toc-number">6.4.</span> <span class="toc-text">4. DualPipe &amp; EPLB</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-3FS"><span class="toc-number">6.5.</span> <span class="toc-text">5. 3FS</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">7.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="post-content"><p>前面提到，CPU运算的瓶颈往往不在于计算，而在于内存、存储和网络。相比于设计算法模型，CPU计算更侧重于工程。</p>
<p>相比下，GPU运算才是真正的高性能运算，虽然它的瓶颈同样可能来自GPU、显存、存储和网络，但提高计算能力、设计优秀的算法、编写高性能低开销的算子，是GPU运算的核心。GPU运算的典型场景就是大模型。</p>
<span id="more"></span>

<h3 id="GPU-运算"><a href="#GPU-运算" class="headerlink" title="GPU 运算"></a>GPU 运算</h3><p>前面提到，CPU 通过SIMD提供了向量化引擎，适用于OLAP存储和OLAP少量维度的数据分析。但如果是以下场景，就需要GPU了。</p>
<ol>
<li>数据维度高，相比OLAP往往存储二维结构化数据，且列数较少；GPU处理的数据维度高（例如图片数据）、或者是非结构化数据（如自然语言序列）</li>
<li><strong>需要高性能矩阵运算</strong>，如矩阵乘法、矩阵加法。</li>
<li>计算单元<strong>用有向无环图的形式组织</strong>，当前运算单元的输出是下一运算单元的输入，计算层很深，计算量庞大</li>
<li>指令数量和条件分支判断数量少</li>
<li>指令数量少意味着不用经常访问内存，cache数量少，无须多级cache。（GPU访问显存的频率远低于CPU访问cache的频率）</li>
</ol>
<h4 id="CUDA-GPU并行编程模型"><a href="#CUDA-GPU并行编程模型" class="headerlink" title="CUDA GPU并行编程模型"></a>CUDA GPU并行编程模型</h4><p>CUDA（Compute Unified Device Architecture）是NVIDIA为GPU设计的并行计算平台和编程模型。nvidia gpu物理上使用​​CUDA核心（FP32&#x2F;INT32）​​执行浮点和整数运算, 每一个Cuda Core由1个浮点数单元FPU和1个逻辑运算单元ALU组成。除了cuda Core，nivida还用张量核Tensor Core模块用于执行融合乘法加法。</p>
<p>cuda的thread是最小计算单元, 用来处理单个数据。多个thread组成block，可用来执行矩阵处理。多个线程块的集合组成grid，用来表示一个大规模计算任务。cuda 引擎会调度thread&#x2F;block&#x2F;grid上的计算到合适的GPU硬件单元上执行。</p>
<p>向量加法的例子，用一个block执行向量加法</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 向量加法核函数</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">addKernel</span><span class="params">(<span class="type">float</span> *a, <span class="type">float</span> *b, <span class="type">float</span> *c, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="keyword">if</span> (i &lt; n) &#123;</span><br><span class="line">        c[i] = a[i] + b[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 调用：addKernel&lt;&lt;&lt;ceil(n/256), 256&gt;&gt;&gt;(a, b, c, n);</span></span><br></pre></td></tr></table></figure>

<h4 id="pytorch-深度学习框架"><a href="#pytorch-深度学习框架" class="headerlink" title="pytorch 深度学习框架"></a>pytorch 深度学习框架</h4><p>PyTorch 是由 Facebook AI Research (FAIR) 开发的开源深度学习框架，以其 ​​动态计算图​​、​​易用性​​ 和 ​​高效的 GPU 加速​​ 著称，广泛应用于学术研究、工业界模型开发和部署。</p>
<p>python前端接口调用, 文档: <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">3</span>)  <span class="comment"># 创建张量</span></span><br><span class="line">y = x.cuda()           <span class="comment"># 移动到 GPU</span></span><br><span class="line">z = y + <span class="number">1</span>              <span class="comment"># GPU 加速计算</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x**<span class="number">2</span> + <span class="number">3</span>*x + <span class="number">1</span></span><br><span class="line">y.backward()           <span class="comment"># 计算梯度 d(y)/d(x) = 2x + 3</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)          <span class="comment"># 输出: 7.0</span></span><br></pre></td></tr></table></figure>

<p>后端C++引擎libtorch, 可以直接基于libtorch进行C++开发。</p>
<ol>
<li>​ATen 库​​：核心张量计算库，支持 CPU&#x2F;GPU 统一代码。</li>
<li>​TorchScript​​：将 Python 模型转换为静态计算图（ScriptModule），用于高性能推理。</li>
<li>​​CUDA 集成​​：通过 c10::cuda 实现低延迟 GPU 操作。</li>
</ol>
<p>pytorch 可以很容易调用cuda开发的算子，参考 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/595851188">https://zhuanlan.zhihu.com/p/595851188</a></p>
<p>代码结构</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">├── ops</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── ops_py</span><br><span class="line">│   │   ├── __init__.py</span><br><span class="line">│   │   └── sum.py</span><br><span class="line">│   └── src</span><br><span class="line">│       ├── reduce_sum</span><br><span class="line">│       │   ├── sum.cpp</span><br><span class="line">│       │   └── sum_cuda.cu</span><br><span class="line">│       └── sum_two_arrays</span><br><span class="line">│           ├── two_sum.cpp</span><br><span class="line">│           └── two_sum_cuda.cu</span><br><span class="line">├── README.md</span><br><span class="line">├── setup.py</span><br><span class="line">└── test_ops.py</span><br></pre></td></tr></table></figure>

<p>src&#x2F;sum_two_arrays&#x2F;two_sum_cuda.cu</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstdio&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> THREADS_PER_BLOCK 256</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> WARP_SIZE 32</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DIVUP(m, n) ((m + n - 1) / n)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">two_sum_kernel</span><span class="params">(<span class="type">const</span> <span class="type">float</span>* a, <span class="type">const</span> <span class="type">float</span>* b, <span class="type">float</span> * c, <span class="type">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> idx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="keyword">if</span> (idx &lt; n)&#123;</span><br><span class="line">        c[idx] = a[idx] + b[idx];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">two_sum_launcher</span><span class="params">(<span class="type">const</span> <span class="type">float</span>* a, <span class="type">const</span> <span class="type">float</span>* b, <span class="type">float</span>* c, <span class="type">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="function">dim3 <span class="title">blockSize</span><span class="params">(DIVUP(n, THREADS_PER_BLOCK))</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">threadSize</span><span class="params">(THREADS_PER_BLOCK)</span></span>;</span><br><span class="line">    two_sum_kernel&lt;&lt;&lt;blockSize, threadSize&gt;&gt;&gt;(a, b, c, n);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>src&#x2F;sum_two_arrays&#x2F;two_sum.cpp</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/serialize/tensor.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CUDA(x) \</span></span><br><span class="line"><span class="meta">  TORCH_CHECK(x.type().is_cuda(), #x, <span class="string">&quot; must be a CUDAtensor &quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CONTIGUOUS(x) \</span></span><br><span class="line"><span class="meta">  TORCH_CHECK(x.is_contiguous(), #x, <span class="string">&quot; must be contiguous &quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_INPUT(x) \</span></span><br><span class="line"><span class="meta">  CHECK_CUDA(x);       \</span></span><br><span class="line"><span class="meta">  CHECK_CONTIGUOUS(x)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">two_sum_launcher</span><span class="params">(<span class="type">const</span> <span class="type">float</span>* a, <span class="type">const</span> <span class="type">float</span>* b, <span class="type">float</span>* c, <span class="type">int</span> n)</span></span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">two_sum_gpu</span><span class="params">(at::Tensor a_tensor, at::Tensor b_tensor, at::Tensor c_tensor)</span></span>&#123;</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(a_tensor);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(b_tensor);</span><br><span class="line">    <span class="built_in">CHECK_INPUT</span>(c_tensor);</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">float</span>* a = a_tensor.<span class="built_in">data_ptr</span>&lt;<span class="type">float</span>&gt;();</span><br><span class="line">    <span class="type">const</span> <span class="type">float</span>* b = b_tensor.<span class="built_in">data_ptr</span>&lt;<span class="type">float</span>&gt;();</span><br><span class="line">    <span class="type">float</span>* c = c_tensor.<span class="built_in">data_ptr</span>&lt;<span class="type">float</span>&gt;();</span><br><span class="line">    <span class="type">int</span> n = a_tensor.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="built_in">two_sum_launcher</span>(a, b, c, n);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m) &#123;</span><br><span class="line">  m.<span class="built_in">def</span>(<span class="string">&quot;forward&quot;</span>, &amp;two_sum_gpu, <span class="string">&quot;sum two arrays (CUDA)&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) { m.def(“forward”, &amp;two_sum_gpu …)} 表示</p>
<ol>
<li>pybind11 宏，声明一个 Python 模块，并创建模块对象</li>
<li>将 C++ 函数 two_sum_gpu 绑定到 Python 模块，并命名为 forward。</li>
</ol>
<p>使用setup.py编译（也可以使用jit编译）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> find_packages, setup</span><br><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> BuildExtension, CUDAExtension</span><br><span class="line"></span><br><span class="line">setup(</span><br><span class="line">    name=<span class="string">&#x27;CudaDemo&#x27;</span>,</span><br><span class="line">    packages=find_packages(),</span><br><span class="line">    version=<span class="string">&#x27;0.1.0&#x27;</span>,</span><br><span class="line">    author=<span class="string">&#x27;xxx&#x27;</span>,</span><br><span class="line">    ext_modules=[</span><br><span class="line">        CUDAExtension(</span><br><span class="line">            <span class="string">&#x27;sum_double&#x27;</span>,</span><br><span class="line">            [<span class="string">&#x27;./ops/src/sum_two_arrays/two_sum.cpp&#x27;</span>,</span><br><span class="line">             <span class="string">&#x27;./ops/src/sum_two_arrays/two_sum_cuda.cu&#x27;</span>,]</span><br><span class="line">        ),</span><br><span class="line">    ],</span><br><span class="line">    cmdclass=&#123;</span><br><span class="line">        <span class="string">&#x27;build_ext&#x27;</span>: BuildExtension</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>pytorch调用cuda算子  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ops/ops_py/sum.py</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Function</span><br><span class="line"><span class="keyword">import</span> sum_double</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SumDouble</span>(<span class="title class_ inherited__">Function</span>):</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, array1, array2</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;sum_double function forward.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            array1 (torch.Tensor): [n,]</span></span><br><span class="line"><span class="string">            array2 (torch.Tensor): [n,]</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            ans (torch.Tensor): [n,]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        array1 = array1.<span class="built_in">float</span>()</span><br><span class="line">        array2 = array2.<span class="built_in">float</span>()</span><br><span class="line">        ans = array1.new_zeros(array1.shape)</span><br><span class="line">        sum_double.forward(array1.contiguous(), array2.contiguous(), ans)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, g_out</span>):</span><br><span class="line">        <span class="comment"># return None, None   # if the function is no need for backpropogation</span></span><br><span class="line"></span><br><span class="line">        g_in1 = g_out.clone()</span><br><span class="line">        g_in2 = g_out.clone()</span><br><span class="line">        <span class="keyword">return</span> g_in1, g_in2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sum_double_op = SumDouble.apply</span><br></pre></td></tr></table></figure>

<h4 id="cudnn和nccl"><a href="#cudnn和nccl" class="headerlink" title="cudnn和nccl"></a>cudnn和nccl</h4><p>cuDNN​​（CUDA Deep Neural Network Library）是英伟达推出的专为深度学习设计的GPU加速库。它针对深度神经网络中的核心操作（如卷积、池化、归一化等）提供高度优化的实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动cudnn加速</span></span><br><span class="line">torch.backends.cudnn.enabled = <span class="literal">True</span>        <span class="comment"># 全局启用</span></span><br><span class="line">torch.backends.cudnn.benchmark = <span class="literal">True</span>      <span class="comment"># 允许自动选择最优算法（固定输入大小时启用）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 混和精度训练</span></span><br><span class="line"><span class="keyword">with</span> torch.cuda.amp.autocast():</span><br><span class="line">    outputs = model(inputs)</span><br></pre></td></tr></table></figure>

<p>​​NCCL（NVIDIA Collective Communications Library）​​ 是 NVIDIA 开发的 ​​GPU 专用通信库​​，旨在优化多 GPU 和多节点间的数据传输效率。cuDNN 可与 NCCL 结合实现多卡通信。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank])</span><br></pre></td></tr></table></figure>

<h3 id="transformer和self-attention"><a href="#transformer和self-attention" class="headerlink" title="transformer和self-attention"></a>transformer和self-attention</h3><p>以上讲了cpu适合通用计算，也就是cpu需要能够各种任务，包括进程任务，cache和内存读写，IO存储，需要具备中断处理能力，就像工作的人一样。CPU还提供了向量化指令增加向量数据处理的能力。GPU适合专门处理矩阵浮点型运算，这种矩阵浮点运算最显著的场景就是深度学习和大模型运算。</p>
<p>目前大模型基本是基于transformer和self-attention架构开发，相关分析 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/624740065">https://zhuanlan.zhihu.com/p/624740065</a> 比较全面，我整理下总结</p>
<ol>
<li>每个transformer层的参数量为12h^2,训练时每个参数占20字节，推理时每个参数占2个字节。h是隐藏层维度</li>
<li>每个参数前向计算需要2个浮点数运算（相当于一次乘法一次加法），反向需要4个浮点数运算（反向需要根据误差计算梯度，然后根据梯度更新权重，需要两次运算，计算量是前向计算的2倍）。一般采用激活重计算技术降低中间激活层的内存占用，这样子又需要一次前向计算。每个参数总计需要8个浮点数运算。推理时每个参数只要2个浮点数运算。</li>
<li>显存占用主要由1. 模型参数 2. 前向计算过程产生的中间激活 3. 后向计算得到的梯度 4.优化器状态四个方面组成。其中模型参数、后向计算得到的梯度、优化器状态参数之和只和隐藏层size有关，而前向计算过程产生的中间激活参数和batchsize、序列长度有关。可以通过减少batchsize，额外的激活重计算来降低中间激活参数。</li>
</ol>
<h4 id="1-模型参数"><a href="#1-模型参数" class="headerlink" title="1. 模型参数"></a>1. 模型参数</h4><p><img src="/../images/self-attention.png" alt="self-attention"></p>
<ol>
<li>transformer由l层组成，每层分为self-attention（多头注意力）和MLP两部分。self-attention块包含3个QKV权重矩阵和一个输出权重矩阵，每个矩阵维度[h, h], 加上偏置参数量为4h^2+4h, h为隐藏层维度</li>
<li>MLP块由两个线性层组成，第一个线性层维度为[h, 4h], 第二个线性层维度为[4h, h], 加上偏置参数量为8h^2+5h</li>
<li>self-attention块和MLP块之后各有一个layer norm层，包含两个参数，缩容参数alpha和偏置参数beta，2个layer norm参数量合计为4h</li>
</ol>
<p>综上, 每个transformer层参数量为12h^2+13h, 对于l层transformer, 参数了近似为12lh^2</p>
<p><img src="/../images/transformer_params.png" alt="transformer_params"></p>
<h4 id="2-参数显存分析"><a href="#2-参数显存分析" class="headerlink" title="2. 参数显存分析"></a>2. 参数显存分析</h4><p>略</p>
<h4 id="3-kvcache分析"><a href="#3-kvcache分析" class="headerlink" title="3. kvcache分析"></a>3. kvcache分析</h4><p>加入kvcache后，一个典型的大模型生成式推断包含了两个阶段：</p>
<ol>
<li>预填充阶段：输入一个prompt序列，为每个transformer层生成 key cache和value cache（KV cache）。</li>
<li>解码阶段：使用并更新KV cache，一个接一个地生成词，当前生成的词依赖于之前已经生成的词。</li>
</ol>
<p>kvcache 只影响第一步QKV矩阵的生成，将矩阵-矩阵乘法 降低为矩阵-向量乘法，减少参数量加速计算</p>
<p><img src="/../images/kvcache.png" alt="kvcache"></p>
<p>KV Cache是Transformer推理性能优化的一项重要工程化技术，各大推理框架都已实现并将其进行了封装。可以看这篇文章 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/63083259">https://zhuanlan.zhihu.com/p/63083259</a></p>
<h3 id="大模型加速"><a href="#大模型加速" class="headerlink" title="大模型加速"></a>大模型加速</h3><p>大模型加速，主要分为模型侧、计算侧和内存IO侧。模型侧主要是压缩和量化，计算侧包括并行计算，cuda算子优化以及MOE训练等，内存IO侧主要在内存分配，共享，高性能存储网络等。</p>
<h4 id="1-模型压缩"><a href="#1-模型压缩" class="headerlink" title="1. 模型压缩"></a>1. 模型压缩</h4><ol>
<li><p>剪裁(Pruning), 核心思想是尽可能保证模型精度不受影响下减少网络的参数量，例如减少网络中神经元的数量<br>剪裁中常用的步骤1. 预训练大模型 2. 修剪网络, 训练小模型 3. 通过微调恢复剪裁对模型的损耗</p>
</li>
<li><p>量化Quantization<br>量化的基本思想是将浮点计算替换成更低比特的计算，从而降低模型体积加快推理速度。量化可以采用定点近似（直接缩小位宽降低精度）和范围近似（通过统计学缩放映射浮点数，需要量化和反量化，精度较高）。</p>
</li>
<li><p>知识蒸馏(Knowledge Distillation)<br>一种教师-学生的训练结构，通常是已训练好的教师模型提供知识，学生模型通过蒸馏训练来获取知识。将教师模型的输出作为软标签与学生模型的软预测计算蒸馏损失，将真实的硬标签与学生模型的硬预测计算学生损失，最终将两种损失结合训练学生模型</p>
</li>
</ol>
<h4 id="2-计算侧加速"><a href="#2-计算侧加速" class="headerlink" title="2. 计算侧加速"></a>2. 计算侧加速</h4><ol>
<li>并行计算<br>数据并行（数据集拆分），流水线并行（模型拆分成子模型），张量并行（模型按层拆分），专家并行（MOE）</li>
</ol>
<p>其中流水线并行是GPU内存不足的无奈之举，各层之间仍然是顺序执行的，并不能加速模型的运算。</p>
<p>张量并行可以使用nivida的Megatron库，将模型内部改为ColumnParallelLinear, ParallelMLP, ParallelAttention等结构</p>
<p>专家并行特指MOE训练（混和专家模型）</p>
<ol start="2">
<li><p>kvcache<br>加速KQV矩阵的生产运算</p>
</li>
<li><p>cuda优化和算子融合<br>cuda 执行矩阵乘法，激活函数，softmax等，每个操作都对应一次cuda调用。可以自定义cuda Attention优化，以及将多个cuda算子融合到一起，减少cuda调用次数，提高性能。</p>
</li>
</ol>
<h4 id="内存和IO优化"><a href="#内存和IO优化" class="headerlink" title="内存和IO优化"></a>内存和IO优化</h4><ol>
<li><p>FlashAttention<br>加速注意力计算并减少内存占用。FlashAttention的核心原理是通过<strong>将输入分块</strong>并在每个块上执行注意力操作，从而减少对高带宽内存（HBM）的读写操作。参考文章 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/676655352">https://zhuanlan.zhihu.com/p/676655352</a></p>
</li>
<li><p>FlashDEcoding<br>FlashAttention对batch size和query length进行了并行化加速，Flash-Decoding在此基础上增加了一个新的并行化维度：keys&#x2F;values的序列长度。即使batch size很小，但只要上下文足够长，它就可以充分利用GPU。</p>
</li>
<li><p>Continuous Batching<br>一个批次中，某些请求可能会比其他请求提前“完成”，但这些完成的请求需要等待整个批次完成才释放资源。<br>Continuous Batching 不会等待批次中的每个序列完成生成，而是实现迭代级调度，一旦批处理中的序列完成生成，就可以在其位置插入新序列，不必等待整个批次完成。 参考文章 <a target="_blank" rel="noopener" href="https://github.com/PaddleJitLab/CUDATutorial/blob/develop/docs/13_continuous_batch/README.md">https://github.com/PaddleJitLab/CUDATutorial/blob/develop/docs/13_continuous_batch/README.md</a></p>
</li>
<li><p>PagedAttention<br>现有的推理系统将 KV Cache 存储在连续的显存空间中，导致显存碎片浪费，以及显存无法共享。</p>
</li>
</ol>
<p>PagedAttention 将 KV cache 组织成了固定大小的 KV blocks，类似虚拟内存中的页。管理显存的分配，同时对推理的重复计算内存共享。</p>
<h3 id="大模型推理框架"><a href="#大模型推理框架" class="headerlink" title="大模型推理框架"></a>大模型推理框架</h3><h4 id="vLLM"><a href="#vLLM" class="headerlink" title="vLLM"></a>vLLM</h4><p>vLLM 是一个快速、易于使用的 LLM 推理和服务库。可以接收流式的处理请求，并调度GPU和模型执行推理和输出</p>
<ol>
<li>调度器<br>在每1个推理阶段，决定要把哪些数据送给模型做推理，同时负责给这些模型分配KV Cache物理块。</li>
<li>Worker<br>CacheEngine：负责管控gpu&#x2F;cpu上的KV cache物理块（调度器的block manager只负责物理块id的分配）<br>Worker.model：负责加载模型，并执行推理。</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/691045737">https://zhuanlan.zhihu.com/p/691045737</a></p>
<h4 id="TensorRT"><a href="#TensorRT" class="headerlink" title="TensorRT"></a>TensorRT</h4><p>​​TensorRT​​ 是 NVIDIA 推出的 ​​高性能深度学习推理优化器，相比vLLM主要从调度层和计算&#x2F;内存资源分配层做优化，TensorRT 主要在cuda和硬件层进行优化。</p>
<ol>
<li>层融合（Layer Fusion）​​：合并卷积、激活、归一化等连续操作为单一内核，减少内存访问开销。</li>
<li>​​INT8​​：通过量化感知训练或校准集动态量化，速度提升 2-4 倍。</li>
<li>​Dynamic Tensor Memory 在每个tensor的使用期间，TensorRT会为其指定显存，避免显存重复申请，减少内存占用和提高重复使用效率。</li>
<li>Multi stream execution 使用CUDA中的stream技术，最大化实现并行操作。</li>
</ol>
<h3 id="MOE训练"><a href="#MOE训练" class="headerlink" title="MOE训练"></a>MOE训练</h3><p>混合专家模型（Mixture of Experts, MOE）训练​​ 是一种通过动态路由机制将输入分配给多个子网络（专家）的高效模型架构，在大规模模型中广泛应用</p>
<ol>
<li>​​专家网络​​, 多个独立的子模型（如全连接层、Transformer块），每个专家专注不同特征模式。可以实现并行处理</li>
<li>​​门控网络（Gating）​​	根据输入生成权重，决定各专家的贡献比例（稀疏或软选择）</li>
<li>​​路由策略​​	控制输入如何分配给专家（如 Top-K 选择、负载均衡约束）。</li>
</ol>
<p>优势</p>
<ol>
<li>​模型容量扩展​​，混和专家模型每次只计算局部的参数（激活局部的专家），因此能在GPU有限情况下训练大量参数的模型。增加专家数量可提升模型能力，而计算量仅随激活的专家数增长。</li>
<li>​​稀疏计算​​：仅部分专家参与推理（如 K&#x3D;2），适合资源受限场景（如 GPU 显存优化）。</li>
<li>​多模态学习​​：不同专家可处理不同类型输入（文本、图像等）。</li>
</ol>
<p>参考文章，<a target="_blank" rel="noopener" href="https://huggingface.co/blog/moe">https://huggingface.co/blog/moe</a></p>
<h3 id="deepseek开源"><a href="#deepseek开源" class="headerlink" title="deepseek开源"></a>deepseek开源</h3><h4 id="1-FlashMLA"><a href="#1-FlashMLA" class="headerlink" title="1. FlashMLA"></a>1. FlashMLA</h4><p>deepseek 借鉴了FlashAttention项目中的一些理念，针对MLA进行优化的CUDA内核算子，并可集成到pytorch</p>
<p>链接 <a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/FlashMLA">https://github.com/deepseek-ai/FlashMLA</a></p>
<p>文档, <a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/FlashMLA/blob/main/docs/20250422-new-kernel-deep-dive.md">https://github.com/deepseek-ai/FlashMLA/blob/main/docs/20250422-new-kernel-deep-dive.md</a></p>
<h4 id="2-DeepEP"><a href="#2-DeepEP" class="headerlink" title="2. DeepEP"></a>2. DeepEP</h4><p>DeepEP 是一个专门为混合专家（MoE）模型和专家并行（EP）设计的通信库。DeepEP is a communication library tailored for Mixture-of-Experts (MoE) and expert parallelism (EP). It provides high-throughput and low-latency all-to-all GPU kernels, which are also as known as MoE dispatch and combine. The library also supports low-precision operations, including FP8.</p>
<p>链接 <a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DeepEP">https://github.com/deepseek-ai/DeepEP</a></p>
<h4 id="3-DeepGEMM"><a href="#3-DeepGEMM" class="headerlink" title="3. DeepGEMM"></a>3. DeepGEMM</h4><p>通用矩阵乘法算子</p>
<h4 id="4-DualPipe-EPLB"><a href="#4-DualPipe-EPLB" class="headerlink" title="4. DualPipe &amp; EPLB"></a>4. DualPipe &amp; EPLB</h4><p>DualPipe 训练时流水线调度，采用了一种独特的调度策略，使得前向传播和反向传播可以在不同的GPU上同时进行<br>EPLB 实现专家负载均衡</p>
<h4 id="5-3FS"><a href="#5-3FS" class="headerlink" title="5. 3FS"></a>5. 3FS</h4><p>为大模型推理提供数据集和模型读写能力，主要关注小IO和大IO读，以及大IO写。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>CPU运算的适用于逻辑运算（如数据清洗），简单整数和向量计算（例如OLAP），而GPU运算适用于矩阵运算（如卷积、矩阵乘法等）。一般的计算模型是GPU-CUDA-Pytorch三件套。</p>
<p>大模型基于transformer构建，是基于多层矩阵运算的复杂模型。本文分析了transformer一层的参数量，计算量，显存使用量和kvcache、重计算加速等。注意大模型和传统流式处理(如Flink的区别)是，大模型是高度密集型矩阵运算，而Flink流式处理的运算并不复杂，侧重于数字或向量计算而非矩阵运算。但大模型推理后续替代Flink犹未可知。</p>
<p>对于大模型加速，主要分为模型侧、计算侧和内存IO侧。模型侧主要是压缩和量化，计算侧包括并行计算，cuda算子优化以及MOE训练等，内存IO侧主要在内存分配，共享，高性能存储网络等。</p>
<p>大模型推理框架，vLLM是一套推理工程的解决方案。最后deepseek开源了包括矩阵运算、MLP算子、高性能网络、MOE训练负责均衡以及高性能存储的项目，都很值得学习。</p>
<p>可以把GPU训练看到类似IO存储，以后的业务层</p>
<ol>
<li>接收网络请求，准备执行环境</li>
<li>执行顺序和if-else逻辑，包括鉴权、流控、日志等</li>
<li>执行IO存储逻辑，包括写数据库&#x2F;写文件&#x2F;写缓存&#x2F;写oss等</li>
<li>执行在线大数据处理逻辑，例如搜广推等（CPU计算）</li>
<li>执行大模型GPU推理逻辑，推理生成序列或预测结果</li>
<li>获得3,4,5的结果，返回给客户端</li>
</ol>
<p>业务层本身是无状态的CPU计算，主要关注的</p>
<ol>
<li>接收海量请求，也就是高并发</li>
<li>自身可以水平扩展</li>
<li>明确后台的能力，为数据库层，模型推理层提供缓存，流控，队列等，防止后端压力过大</li>
<li>复杂的业务逻辑解耦</li>
</ol>
<p>IO存储、在线大数据处理、大模型推理层负责提供高并发、高性能的存储和计算推理服务。</p>
<p>by the way, 显然实时性的业务更具有挑战性，需要低延迟、高吞吐和高QOS。例如后端业务层开发（毫无疑问要是实时返回的）、搜索广告推荐（需要实时数据分析）、大模型推理（需要实时推理）、分布式数据库和存储（需要提供实时表和文件读写服务）、量化交易（需要低延迟自动化和手段触发策略）等。如果某业务无实时性处理要求，那技术性将会大打折扣。</p>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/compute/" rel="tag">compute</a></li></ul></div><div class="post-nav"><a class="pre" href="/2025/07/06/%E5%AD%98%E5%82%A8/brpc(1)%E2%80%94bthread%E5%92%8Cbrpc/">brpc(1)—bthread和brpc</a><a class="next" href="/2025/04/27/%E8%AE%A1%E7%AE%97/%E8%AE%A1%E7%AE%97(1)%E2%80%94CPU%E8%AE%A1%E7%AE%97%E5%92%8C%E5%A4%A7%E6%95%B0%E6%8D%AE/">计算(1)——CPU计算和大数据</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://larrystd.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>To be a better man.</p><a class="info-icon" href="https://twitter.com/username" title="Twitter" target="_blank" style="margin-inline:5px"> <i class="fa fa-twitter-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:admin@domain.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/username" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/application/">application</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/compute/">compute</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hello/">hello</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/language/">language</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/network/">network</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/storage/">storage</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/hello/" style="font-size: 15px;">hello</a> <a href="/tags/network/" style="font-size: 15px;">network</a> <a href="/tags/brpc/" style="font-size: 15px;">brpc</a> <a href="/tags/storage/" style="font-size: 15px;">storage</a> <a href="/tags/leveldb/" style="font-size: 15px;">leveldb</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/redis/" style="font-size: 15px;">redis</a> <a href="/tags/application/" style="font-size: 15px;">application</a> <a href="/tags/base/" style="font-size: 15px;">base</a> <a href="/tags/language/" style="font-size: 15px;">language</a> <a href="/tags/cpp/" style="font-size: 15px;">cpp</a> <a href="/tags/coroutine/" style="font-size: 15px;">coroutine</a> <a href="/tags/compute/" style="font-size: 15px;">compute</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2025/07/06/%E5%AD%98%E5%82%A8/brpc(1)%E2%80%94bthread%E5%92%8Cbrpc/">brpc(1)—bthread和brpc</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/05/03/%E8%AE%A1%E7%AE%97/%E8%AE%A1%E7%AE%97(2)%E2%80%94GPU%E8%AE%A1%E7%AE%97%E5%92%8C%E5%A4%A7%E6%A8%A1%E5%9E%8B/">计算(2)——GPU计算和大模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/04/27/%E8%AE%A1%E7%AE%97/%E8%AE%A1%E7%AE%97(1)%E2%80%94CPU%E8%AE%A1%E7%AE%97%E5%92%8C%E5%A4%A7%E6%95%B0%E6%8D%AE/">计算(1)——CPU计算和大数据</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/04/22/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E2%80%94C++%E5%8F%B3%E5%80%BC%E5%92%8C%E5%8F%B3%E5%80%BC%E5%BC%95%E7%94%A8/">编程语言——C++右值和右值引用</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/04/20/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E2%80%94C++%E5%8D%8F%E7%A8%8B%E5%92%8C%E9%AB%98%E6%80%A7%E8%83%BD%E7%BC%96%E7%A8%8B/">编程语言——C++协程和高性能编程</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/04/07/%E5%AD%98%E5%82%A8/%E5%AD%98%E5%82%A8%E2%80%94%E8%B0%88%E5%AD%98%E5%82%A8%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/">存储——谈存储文件系统</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/15/%E5%AD%98%E5%82%A8/leveldb(2)%E2%80%94%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/">leveldb(2)—线程模型和并发控制</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/12/%E5%AD%98%E5%82%A8/redis(2)%E2%80%94%E7%BD%91%E7%BB%9C%E5%A4%84%E7%90%86%E5%92%8C%E6%8C%81%E4%B9%85%E5%8C%96/">redis(2)——网络处理和持久化</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/10/%E5%AD%98%E5%82%A8/redis(1)%E2%80%94%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">redis(1)——数据结构</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/01/31/%E5%AD%98%E5%82%A8/leveldb(1)%E2%80%94%E6%A6%82%E8%A7%88/">leveldb(1)—概览</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2026 <a href="/." rel="nofollow">Infinity Code.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="https://unpkg.com/@fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="https://unpkg.com/@fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>